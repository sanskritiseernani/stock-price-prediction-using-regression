{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs383Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF9CKWIKUinT",
        "outputId": "db4f6014-0109-42a7-b9a4-a23ca714431d"
      },
      "source": [
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import random\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import median_absolute_error as mae\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK1RAb8YaK-_"
      },
      "source": [
        "def std_data(X, mean, std):\n",
        "    #  Standardizes the data (except for the last column of course) using the training data\n",
        "    sX = (X - mean)/std\n",
        "    return sX\n",
        "\n",
        "def Add_Bias(X):\n",
        "    # Add in Bias\n",
        "    new_X = []\n",
        "    for arr in  X:\n",
        "        new_X.append(np.append([1], arr))\n",
        "    new_X = np.array(new_X)\n",
        "    return new_X\n",
        "\n",
        "def read_data(input_file):\n",
        "    with open(input_file) as csvfile:\n",
        "        readCSV = csv.reader(csvfile)\n",
        "        data = list(readCSV)\n",
        "    datanp = np.array(data[1:])\n",
        "    data = []\n",
        "    for i, row in enumerate(datanp):\n",
        "      c = []\n",
        "      for j, col in enumerate(row):\n",
        "        if col != '':\n",
        "          c.append(float(col))\n",
        "      if c != []:\n",
        "        data.append(c)\n",
        "    data = np.array(data)\n",
        "    label = data[:,-1]\n",
        "    data = data[:, 0:-1]\n",
        "    return np.array(data), np.array(label)\n",
        "\n",
        "def Classifier(Y_old):\n",
        "    Y_new = []\n",
        "    for y in Y_old:\n",
        "        if y < 0.5:\n",
        "            Y_new.append(0)\n",
        "        else:\n",
        "            Y_new.append(1)\n",
        "    return np.array(Y_new)\n",
        "\n",
        "def precision(tp, fp):\n",
        "    print(tp, fp)\n",
        "    return tp/(tp+fp)\n",
        "\n",
        "def recall(tp, fn):\n",
        "    return tp/(tp+fn)\n",
        "\n",
        "def f_measure(precision, recall):\n",
        "    return (2*precision*recall)/(precision+recall)\n",
        "\n",
        "def accuracy(tp, tn, overall):\n",
        "    return (tp+tn)/overall\n",
        "\n",
        "def Class_stats(tp_count, tn_count, fp_count, fn_count):\n",
        "    Precision = precision(tp_count, fp_count)\n",
        "    Recall = recall(tp_count, fn_count)\n",
        "    F_measure = f_measure(Precision, Recall)\n",
        "    Accuracy = accuracy(tp_count, tn_count, tp_count + tn_count + fp_count + fn_count)\n",
        "    print(\"\\nPrecision: \", Precision)\n",
        "    print(\"\\nRecall: \", Recall)\n",
        "    print(\"\\nf_measure: \", F_measure)\n",
        "    print(\"\\nAccuracy: \", Accuracy)\n",
        "    \n",
        "def LRSC(X, Y):\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=0)\n",
        "\n",
        "    Y_train = Y_train.reshape(len(Y_train),1)\n",
        "    Y_test = Y_test.reshape(len(Y_test),1)\n",
        "    \n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    std = np.std(X_train, axis=0, ddof=1)\n",
        "    X_train = std_data(X_train, mean, std)\n",
        "    \n",
        "    # Add in Bias\n",
        "    X_train = Add_Bias(X_train)\n",
        "    X_train[np.where(np.isnan(X_train))] = 0\n",
        "    \n",
        "     # Standardize Data\n",
        "    X_test = std_data(X_test, mean, std)\n",
        "    \n",
        "    # Add in Bias\n",
        "    X_test = Add_Bias(X_test)\n",
        "    \n",
        "    N = len(X_train)\n",
        "    D = len(X_train[0])\n",
        "\n",
        "    thetas = []\n",
        "    random.seed(0)\n",
        "    for j in range(1, D+1):\n",
        "        thetas.append([random.uniform(-1, 1)])\n",
        "    thetas = np.array(thetas)\n",
        "    learn_rate = 0.1\n",
        "    \n",
        "    # Applies the solution to the training samples\n",
        "    Y_new = np.dot(X_train, thetas)\n",
        "\n",
        "    # Cost Function\n",
        "    sig = 1/(1 + np.exp(-Y_new))\n",
        "    cost = (1/N)*((-Y_train.T @ np.log(sig +np.finfo(float).eps)) - (1 - Y_train).T @ np.log(1 - sig +np.finfo(float).eps))\n",
        "\n",
        "    cost = -1\n",
        "    new_cost = 0\n",
        "    iterate_count = 1\n",
        "    cost_array = []\n",
        "    while iterate_count <= 150000 and abs(new_cost - cost) >= (2**(-23)):\n",
        "        cost = new_cost\n",
        "        thetas = thetas - ((learn_rate/N) * np.dot(X_train.T, (sig - Y_train)))\n",
        "        Y_new = np.dot(X_train, thetas)\n",
        " \n",
        "        # Cost Function\n",
        "        sig = np.array(1/(1 + np.exp(-Y_new)))\n",
        "        new_cost = (1/N)*((-Y_train.T @ np.log(sig +np.finfo(float).eps)) - (1 - Y_train).T @ np.log(1 - sig +np.finfo(float).eps))\n",
        "        \n",
        "        new_cost = new_cost[0]\n",
        "        iterate_count += 1\n",
        "\n",
        "    Y_new = np.dot(X_test, thetas)\n",
        "    sig = np.array(1/(1 + np.exp(-Y_new)))\n",
        "\n",
        "    Y_new = Classifier(Y_new)\n",
        "\n",
        "    fp_count = 0\n",
        "    tp_count = 0\n",
        "    fn_count = 0\n",
        "    tn_count = 0\n",
        "    for i, y in enumerate(Y_test):\n",
        "        if y == 1:\n",
        "            if y == Y_new[i]:\n",
        "                # true positive\n",
        "                tp_count += 1\n",
        "            else:\n",
        "                # false negative\n",
        "                fn_count += 1\n",
        "        else:\n",
        "            if y == Y_new[i]:\n",
        "                # true negative\n",
        "                tn_count += 1\n",
        "            else:\n",
        "                # false positive\n",
        "                fp_count += 1\n",
        "\n",
        "    print(\"\\nTrue Positives: \", tp_count)\n",
        "    print(\"True Negatives: \", tn_count)\n",
        "    print(\"False Positives: \", fp_count)\n",
        "    print(\"False Negatives: \", fn_count)\n",
        "    Class_stats(tp_count, tn_count, fp_count, fn_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wszEuYu0nc"
      },
      "source": [
        "def Normal_Model(input_class):\n",
        "    mean = np.mean(input_class, axis=0)\n",
        "    std = np.std(input_class, axis=0, ddof=1)\n",
        "    return mean, std\n",
        "\n",
        "def Split_Spam_Nonspam(X_train, Y_train):\n",
        "    spam_x = []\n",
        "    nonspam_x = []\n",
        "    for i, y in enumerate(Y_train):\n",
        "        if y == 0:\n",
        "            nonspam_x.append(X_train[i])\n",
        "        elif y == 1:\n",
        "            spam_x.append(X_train[i])\n",
        "    spam_x = np.array(spam_x)\n",
        "    nonspam_x = np.array(nonspam_x)\n",
        "    return spam_x, nonspam_x\n",
        "\n",
        "def NBC(X,Y):\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=0)\n",
        "\n",
        "    Y_train = Y_train.reshape(len(Y_train),1)\n",
        "    Y_test = Y_test.reshape(len(Y_test),1)\n",
        "    \n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    std = np.std(X_train, axis=0, ddof=1)\n",
        "    \n",
        "    # Standardize Data\n",
        "    X_train = std_data(X_train, mean, std)\n",
        "    \n",
        "    # Add in Bias  \n",
        "    X_train = Add_Bias(X_train)\n",
        "    \n",
        "    # Standardize Data\n",
        "    X_test = std_data(X_test, mean, std)\n",
        "    \n",
        "    # Add in Bias  \n",
        "    X_test = Add_Bias(X_test)\n",
        "    \n",
        "    spam_x, nonspam_x = Split_Spam_Nonspam(X_train, Y_train)\n",
        "\n",
        "    spam_mean, spam_std = Normal_Model(spam_x)\n",
        "    spam_normal_models = norm.pdf(X_test, spam_mean, spam_std)\n",
        "    spam_cum_prob = []\n",
        "    where_nan = np.isnan(spam_normal_models)\n",
        "    spam_normal_models[where_nan] = 0.00000000000001\n",
        "    \n",
        "    spam_prior = len(spam_x)/len(X_train)\n",
        "    for row in spam_normal_models:\n",
        "        spam_cum_prob.append(np.prod(row)*spam_prior)\n",
        "    \n",
        "    nonspam_mean, nonspam_std = Normal_Model(nonspam_x)\n",
        "    nonspam_normal_models = norm.pdf(X_test, nonspam_mean, nonspam_std)\n",
        "    nonspam_cum_prob = []\n",
        "    where_nan = np.isnan(nonspam_normal_models)\n",
        "    nonspam_normal_models[where_nan] = 0.00000000000001\n",
        "    \n",
        "    nonspam_prior = len(nonspam_x)/len(X_train)\n",
        "    for row in nonspam_normal_models:\n",
        "        nonspam_cum_prob.append(np.prod(row)*nonspam_prior)\n",
        "\n",
        "    Y_new = []\n",
        "    for i, n in enumerate(spam_cum_prob):\n",
        "        if n >= nonspam_cum_prob[i]:\n",
        "            Y_new.append(1)\n",
        "        else:\n",
        "            Y_new.append(0) \n",
        "    Y_new = np.array(Y_new)\n",
        "    \n",
        "    fp_count = 0\n",
        "    tp_count = 0\n",
        "    fn_count = 0\n",
        "    tn_count = 0\n",
        "    \n",
        "    Y_new = np.array(Y_new)\n",
        "    for i, y in enumerate(Y_test):\n",
        "        if y == 1:\n",
        "            if y == Y_new[i]:\n",
        "                # true positive\n",
        "                tp_count += 1\n",
        "            else:\n",
        "                # false negative\n",
        "                fn_count += 1\n",
        "        else:\n",
        "            if y == Y_new[i]:\n",
        "                # true negative\n",
        "                tn_count += 1\n",
        "            else:\n",
        "                # false positive\n",
        "                fp_count += 1\n",
        "    print(\"\\nTrue Positives: \", tp_count)\n",
        "    print(\"True Negatives: \", tn_count)\n",
        "    print(\"False Positives: \", fp_count)\n",
        "    print(\"False Negatives: \", fn_count)\n",
        "    Class_stats(tp_count, tn_count, fp_count, fn_count) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4VwVYAJw2kW",
        "outputId": "b3e6eb21-08f7-4a05-c60e-90e0805fcc2e"
      },
      "source": [
        "# Quarterly Report Prediction Stock Trends\n",
        "print(\"************ Quarterly Report Prediction Stock Trends ************\\n\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import sklearn.model_selection as cross_val\n",
        "\n",
        "\n",
        "X, Y = read_data('AAPL_STOCK.csv')\n",
        "print(\"Logistic Regression: \\n\")\n",
        "# LRSC(X, Y)\n",
        "predicted = cross_val.cross_val_predict(LogisticRegression(), X, Y, cv=6)\n",
        "print(\"Accuracy: \", metrics.accuracy_score(Y, predicted))\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(Y, predicted, \n",
        "                                                          average=\"weighted\")\n",
        "print(\"Average Precision: \", precision)\n",
        "print(\"Average Recall: \", recall)\n",
        "print(\"Average F_Score: \", f1)\n",
        "print(metrics.classification_report(Y, predicted))\n",
        "\n",
        "print(\"_____________________________________________________________\")\n",
        "print(\"\\nNaive Bayes: \\n\")\n",
        "# NBC(X, Y)\n",
        "predicted = cross_val.cross_val_predict(GaussianNB(), X, Y, cv=6)\n",
        "print(\"Accuracy: \", metrics.accuracy_score(Y, predicted))\n",
        "precision, recall, f1, support = precision_recall_fscore_support(Y, predicted, \n",
        "                                                          average=\"weighted\")\n",
        "print(\"Average Precision: \", precision)\n",
        "print(\"Average Recall: \", recall)\n",
        "print(\"Average F_Score: \", f1)\n",
        "print()\n",
        "print(metrics.classification_report(Y, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************ Quarterly Report Prediction Stock Trends ************\n",
            "\n",
            "Logistic Regression: \n",
            "\n",
            "Accuracy:  0.6818181818181818\n",
            "Average Precision:  0.6267942583732057\n",
            "Average Recall:  0.6818181818181818\n",
            "Average F_Score:  0.6424242424242425\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.33      0.17      0.22         6\n",
            "         1.0       0.74      0.88      0.80        16\n",
            "\n",
            "    accuracy                           0.68        22\n",
            "   macro avg       0.54      0.52      0.51        22\n",
            "weighted avg       0.63      0.68      0.64        22\n",
            "\n",
            "_____________________________________________________________\n",
            "\n",
            "Naive Bayes: \n",
            "\n",
            "Accuracy:  0.5454545454545454\n",
            "Average Precision:  0.5876623376623377\n",
            "Average Recall:  0.5454545454545454\n",
            "Average F_Score:  0.5627705627705627\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.25      0.33      0.29         6\n",
            "         1.0       0.71      0.62      0.67        16\n",
            "\n",
            "    accuracy                           0.55        22\n",
            "   macro avg       0.48      0.48      0.48        22\n",
            "weighted avg       0.59      0.55      0.56        22\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMD9R0eDZ4MZ"
      },
      "source": [
        "# Citation: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "\n",
        "contractions = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaHiZ6InZ4Qu",
        "outputId": "220fd3cc-3e2c-4230-9e1a-bb309d08d4aa"
      },
      "source": [
        "# Citation: https://medium.com/@Currie32/predicting-the-stock-market-with-the-news-and-deep-learning-7fc8f5f639bc\n",
        "# https://github.com/Currie32/Predicting-the-Dow-Jones-with-Headlines/blob/master/Predict_Dow_with_News.ipynb\n",
        "\n",
        "\n",
        "def clean_text(text, remove_stopwords = True):\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'0,0', '00', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r'\\$', ' $ ', text)\n",
        "    text = re.sub(r'u s ', ' united states ', text)\n",
        "    text = re.sub(r'u n ', ' united nations ', text)\n",
        "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
        "    text = re.sub(r'j k ', ' jk ', text)\n",
        "    text = re.sub(r' s ', ' ', text)\n",
        "    text = re.sub(r' yr ', ' year ', text)\n",
        "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
        "    text = re.sub(r'0km ', '0 km ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "with open(\"Combined_News_DJIA.csv\") as csvfile:\n",
        "  readCSV = csv.reader(csvfile)\n",
        "  f = list(readCSV)\n",
        "  size = len(f)\n",
        "  data = []\n",
        "  Y = []\n",
        "  for i, row in enumerate(f):\n",
        "    if i != 0:\n",
        "      c = []\n",
        "      for j, col in enumerate(row):\n",
        "        if j == 1:\n",
        "          Y.append([int(col)])\n",
        "        elif j != 0:\n",
        "          c.append(col)\n",
        "      data.append(c)\n",
        "  Y = np.array(Y)\n",
        "  headlines = np.array(data, dtype=object)\n",
        "\n",
        "# Clean the headlines\n",
        "clean_headlines = []\n",
        "\n",
        "for daily_headlines in headlines:\n",
        "    clean_daily_headlines = []\n",
        "    for headline in daily_headlines:\n",
        "        clean_daily_headlines.append(clean_text(headline))\n",
        "    clean_headlines.append(clean_daily_headlines)\n",
        "\n",
        "# Take a look at some headlines to ensure everything was cleaned well\n",
        "clean_headlines[0]\n",
        "\n",
        "# Find the number of times each word was used and the size of the vocabulary\n",
        "word_counts = {}\n",
        "\n",
        "for date in clean_headlines:\n",
        "    for headline in date:\n",
        "        for word in headline.split():\n",
        "            if word not in word_counts:\n",
        "                word_counts[word] = 1\n",
        "            else:\n",
        "                word_counts[word] += 1\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))\n",
        "\n",
        "# Load GloVe's embeddings\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0]\n",
        "      embedding = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))\n",
        "\n",
        "# Find the number of words that are missing from GloVe, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 10\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from GloVe:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n",
        "\n",
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total Number of Unique Words:\", len(word_counts))\n",
        "print(\"Number of Words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of Words we will use: {}%\".format(usage_ratio))\n",
        "\n",
        "# Need to use 50 for embedding dimensions to match GloVe's vectors.\n",
        "embedding_dim = 50\n",
        "\n",
        "nb_words = len(vocab_to_int)\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in GloVe, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))\n",
        "\n",
        "# Change the text from words to integers\n",
        "# If word is not in vocab, replace it with <UNK> (unknown)\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_headlines = []\n",
        "\n",
        "for date in clean_headlines:\n",
        "    int_daily_headlines = []\n",
        "    for headline in date:\n",
        "        int_headline = []\n",
        "        for word in headline.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                int_headline.append(vocab_to_int[word])\n",
        "            else:\n",
        "                int_headline.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        int_daily_headlines.append(int_headline)\n",
        "    int_headlines.append(int_daily_headlines)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))\n",
        "\n",
        "# Find the length of headlines\n",
        "lengths = []\n",
        "for date in int_headlines:\n",
        "    for headline in date:\n",
        "        lengths.append(len(headline))\n",
        "\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "\n",
        "lengths.describe()\n",
        "\n",
        "# Limit the length of a day's news to 200 words, and the length of any headline to 16 words.\n",
        "# These values are chosen to not have an excessively long training time and \n",
        "# balance the number of headlines used and the number of words from each headline.\n",
        "max_headline_length = 16\n",
        "max_daily_length = 200\n",
        "pad_headlines = []\n",
        "\n",
        "for date in int_headlines:\n",
        "    pad_daily_headlines = []\n",
        "    for headline in date:\n",
        "        # Add headline if it is less than max length\n",
        "        if len(headline) <= max_headline_length:\n",
        "            for word in headline:\n",
        "                pad_daily_headlines.append(word)\n",
        "        # Limit headline if it is more than max length  \n",
        "        else:\n",
        "            headline = headline[:max_headline_length]\n",
        "            for word in headline:\n",
        "                pad_daily_headlines.append(word)\n",
        "    \n",
        "    # Pad daily_headlines if they are less than max length\n",
        "    if len(pad_daily_headlines) < max_daily_length:\n",
        "        for i in range(max_daily_length-len(pad_daily_headlines)):\n",
        "            pad = vocab_to_int[\"<PAD>\"]\n",
        "            pad_daily_headlines.append(pad)\n",
        "    # Limit daily_headlines if they are more than max length\n",
        "    else:\n",
        "        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n",
        "    pad_headlines.append(pad_daily_headlines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 35198\n",
            "Word embeddings: 400000\n",
            "Number of words missing from GloVe: 41\n",
            "Percent of words that are missing from vocabulary: 0.12%\n",
            "Total Number of Unique Words: 35198\n",
            "Number of Words we will use: 31816\n",
            "Percent of Words we will use: 90.39%\n",
            "31816\n",
            "Total number of words in headlines: 617175\n",
            "Total number of UNKs in headlines: 4285\n",
            "Percent of words that are UNK: 0.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fK0pgsJrLoW",
        "outputId": "63961141-c91e-4a72-bba6-50955f45dea8"
      },
      "source": [
        "# News Prediction Stock Trends\n",
        "print(\"************ News Prediction Stock Trends ************\\n\")\n",
        "\n",
        "print(\"Logistic Regression: \")\n",
        "LRSC(pad_headlines, Y)\n",
        "print(\"_____________________________________________________________\")\n",
        "print(\"\\nNaive Bayes: \")\n",
        "NBC(pad_headlines, Y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************ News Prediction Stock Trends ************\n",
            "\n",
            "Logistic Regression: \n",
            "\n",
            "True Positives:  115\n",
            "True Negatives:  193\n",
            "False Positives:  117\n",
            "False Negatives:  238\n",
            "115 117\n",
            "\n",
            "Precision:  0.4956896551724138\n",
            "\n",
            "Recall:  0.32577903682719545\n",
            "\n",
            "f_measure:  0.39316239316239315\n",
            "\n",
            "Accuracy:  0.4645550527903469\n",
            "_____________________________________________________________\n",
            "\n",
            "Naive Bayes: \n",
            "\n",
            "True Positives:  197\n",
            "True Negatives:  142\n",
            "False Positives:  168\n",
            "False Negatives:  156\n",
            "197 168\n",
            "\n",
            "Precision:  0.5397260273972603\n",
            "\n",
            "Recall:  0.5580736543909348\n",
            "\n",
            "f_measure:  0.5487465181058496\n",
            "\n",
            "Accuracy:  0.5113122171945701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BvqtBE2Ru9B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}